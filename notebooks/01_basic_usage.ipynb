{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a91837",
   "metadata": {},
   "source": [
    "# Basic Usage of ML Project Starter\n",
    "\n",
    "This notebook demonstrates the basic usage of our modular ML project structure using a simple classification example with the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a594826",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.preprocess import DataPreprocessor\n",
    "from src.model import MLModel\n",
    "from src.evaluate import ModelEvaluator\n",
    "from utils.helpers import validate_dataframe, safe_file_path, remove_outliers\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089b004",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b6a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation: Validation successful\n",
      "Dataset shape: (150, 5)\n",
      "\n",
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      "First few rows:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n"
     ]
    }
   ],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "data['target'] = iris.target\n",
    "\n",
    "# Validate the dataset\n",
    "required_cols = iris.feature_names + ['target']\n",
    "numeric_cols = iris.feature_names\n",
    "is_valid, message = validate_dataframe(data, required_cols, numeric_cols)\n",
    "print('Data validation:', message)\n",
    "\n",
    "print('Dataset shape:', data.shape)\n",
    "print('\\nFeature names:', iris.feature_names)\n",
    "print('\\nFirst few rows:')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e47974",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce404cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (150, 5)\n",
      "Data shape after outlier removal: (149, 5)\n",
      "Training set shape: (120, 4)\n",
      "Test set shape: (30, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kerod/Documents/10x_starter/ModularOOPStarter/notebooks/../utils/helpers.py:117: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_clean = df_clean[z_scores < n_std]\n",
      "/home/kerod/Documents/10x_starter/ModularOOPStarter/notebooks/../utils/helpers.py:117: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_clean = df_clean[z_scores < n_std]\n"
     ]
    }
   ],
   "source": [
    "# Check for and remove outliers\n",
    "data_clean = remove_outliers(data, iris.feature_names, n_std=3.0)\n",
    "print('Original data shape:', data.shape)\n",
    "print('Data shape after outlier removal:', data_clean.shape)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Preprocess training data\n",
    "X_train, y_train = preprocessor.preprocess_data(train_data, target_column='target')\n",
    "\n",
    "# Preprocess test data\n",
    "X_test, y_test = preprocessor.preprocess_data(test_data, target_column='target')\n",
    "\n",
    "print('Training set shape:', X_train.shape)\n",
    "print('Test set shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca8937",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30cfe228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train model\n",
    "model = MLModel(model_params={\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 5,\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb985e68",
   "metadata": {},
   "source": [
    "## 5. Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50838585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9666666666666667\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.90      1.00      0.95         9\n",
      "           2       1.00      0.91      0.95        11\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = ModelEvaluator()\n",
    "results = evaluator.evaluate_model(y_test, predictions)\n",
    "\n",
    "print('Model Accuracy:', results['accuracy'])\n",
    "print('\\nClassification Report:')\n",
    "print(results['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ce909",
   "metadata": {},
   "source": [
    "## 5. Save and Load Model\n",
    "\n",
    "Demonstrate model persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0140be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ../data/models/iris_model.joblib\n",
      "Loaded Model Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Create a safe path for model saving\\\n",
    "model_path = safe_file_path('../data/models/iris_model.joblib')\n",
    "\n",
    "# Save the model\n",
    "model.save_model(model_path)\n",
    "print(f'Model saved to: {model_path}')\n",
    "\n",
    "# Load the model\n",
    "new_model = MLModel()\n",
    "new_model.load_model('../data/iris_model.joblib')\n",
    "\n",
    "# Verify the loaded model works\n",
    "new_predictions = new_model.predict(X_test)\n",
    "print(\"Loaded Model Accuracy:\", evaluator.evaluate_model(y_test, new_predictions)['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
